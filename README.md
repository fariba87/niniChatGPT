# niniChatGPT
document completer 

ChatGPT as a large-scale project is trained in two main stages:
1) pretraining stage on a very large volumes of internet text data for document completer [decoder only transformer]
2) finetune on a large(but not very large as the first stage) dataset for the task of question answering.[aligning the pretrained model to be like an assistant]
details about the second stage is not publicly available for people,(just available internally for openai),but it is understood for their blog post that,
there are three steps for committing this stage:
	1) start to collect train data[thousands] consisting of question in the upside and answer in the downside and finetune the model on this data[to focus just on question-answering]
	2) let the model in prediction stage to respond to sample questions and different readers rank the answers generated by the model. and after training this reward model,
	3) run PPO policy policy gradient reinforcement learning optimizer

In this project we are focusing on the reimplementation of the first stage but on a tiny dataset, called tiny shakespear.

## tiny shakespear
Dataset Description:

	1 Mega byte 
	total text of shakespear poems in one txt file
	consist of around 1 M characters
	consist of 65 unique character(character vocabulary)
Model Description:

	It was named as nanoGPT in Andrew Karpathy's GitHub page.
	GPT is abbreviated for Generatively Pretrained Transformer, so the main module of this architecture is transformer.
	but how transformer was appeared in the area of deep learning? 
	text synthesis is a kind of  sequence to sequence modeling problem, the first deep learning model for sequences was RNN
        what was wrong with RNN?-> gradient vanishing problem
	what solution came after?->LSTM-> GRU (faster)
	what next to focus on special part of sequence?-> Attention was suggested to pay more attention to special part of sequence. (Attention+RNN)
	what is transformer?->Transformer is just attention without complex architecture for RNN. it is based on simple linear transformation and attention mechanism.
a few notes on Transformer paper:

	it was originally proposed for sequence to sequence modeling (many to many) in machine translation issue.
 	input: French sentence[sequence of words]  output:english sentence[sequence of words]
 	it consists of both encoder and decoder. in both sides attention is applied. besides, there is a cross attention on the decoder side to pay attention again to encoder.
	there are different tricks they used: layer normalization, multi-head attention, residual block, self attention, cross attention,...


## Language Model
what is language model? 

it is a way to model a sequence of words/character/tokens [we will do character level]

types of LM:
1) Bi-gram Language model: where the characters not communicate to each other
2) transformer-based LM : we will train it for our problem

we will train our transformer, so it will learn all the pattern within this text, and learn how the characters follow each other
	
	Note: nonaGPT is not the only approach to train transformer, but it is the simplest


Let's jump into code:
1) ### data preparing 
	as the very first stage of every machine learning project, data to model can not be text but numbers or vectors, so the encoding is a must.
    in the area of text : encoding = tokenizing = convert raw text of string to sequence of numbers
   

      how to do encoding?
       1) simple manual way:	
           find all unique char in all texts
           create a simple dictionary (key,value)=[char,num] for encoder and (value,char) for decoder
       2) encoder proposed by Google: SentencePiece
       3) encoder proposed by OpenAI, which is also used by chatGPT models: Tiktoken
       4) there exist some layer in tensorflow and torch to do that
2) ### data feeding to model 
    how to feed the data to model?
   1) Time Dimension
   
       what is out purpose?-> given a sequence of characters[at least one]: predict the next character
       due to complexity,we will not feed whole text data to the model at once, we will feed it through multiple chunks
       these chunks has some length, and also a maximum length, which we will call it block size in the rest of this article.
       suppose a chunk : [18  47 56 9 16 8 43 21 ] with max length=8
       the model should be trained simultaneously for each of these 8 positions
       then, at inference time, we can generate using as little as one character on context, thereafter, it can predict up to blocksize[since in training stage, transformer never see more than block size]
   2) Batch Dimension
   
       Usually in machine learning project data is fed in batches
       the beginning of each batch is chosen randomly.
   3) overall: many batches of multiple chunks of text: that are all like stack up in a single tensor
       ex: Bs =4 , maxlen=8 ->4*8 =32 we have 32 independent samples that transformer simultaneously process them
3) ### model architecture:
  as I said before we are going to train a transformer based language model
  but Let's start with simple bi-gram language model which is just an MLP.
       without training
       prediction:
       by doing so, characters are not communicating with each other, and the prediction of single character is just based on a single character,
       ex: when you are at position 5 , to predict 6th , the only information  used is 5th

       generate: not good at all

       with train 
       loss: cross entropy
       optimizer: Adam
       N-epoch =1000
       prediction:   B*T*C 
       ground truth : B*T
       note: these should be changed since pytorch cross entropy class accept in a different order
       first step: loss = ln(1/vocabulary size)
	
       generate: better but still randomly

 Now, time for transformer to emerge: (but again step by step we go further)

	1) attention [worst but simplest by averaging]
	2) attention (better way)
	3) (single head)
	4) (multi-head)
	5) Feedforward layer
	6) using multiple blocks of multi-head and FF (challenges: deeper architectures-> suffer form optimization-> possible solutions:)
	    6-1: residual/skip connection
	    6-2: Add and Norm layers,before or after multi-head and FF
	    6-3: using dropout to prevent over-fitting
	7) during each of the above steps validation loss decreases
	8) and that's it: our decoder only transformer
 Now, lets explain in more details the above 8 stages:

	1) Attention:
	Note: we will try to pay attention to the past(not future! because we are going to predict future)[it was not the case for original transformer paper in train stage, but test stage]
	the simplest way to pay attention to the past is "averaging".it is extremely lossy since we lost the positional information
	we implement it using triangular uniform matrix

	2) attention(better way)1:
	   every single token/node, for each B*T tokens independently , emit two vectors:
		1) Query(what i am looking for)
		2) key(what i contained)
		so far, no communication
		wei = Q . K (for every other keys from every other tokens)
		Now : every single batch element , has different wei, because of different tokens in different positions
		we just explained self attention where Q , K where both from the same source
		Note: self attention can not tolerate very high lr

	3) single head : it was a self attention unit

	4) multi head : multiple attention in parallel and concatenating the results (channel wise )

	5) FeedForward : it is computed independently for each tokens

	6) block (multiple times of multi-head+FF)

	


	
  



	



